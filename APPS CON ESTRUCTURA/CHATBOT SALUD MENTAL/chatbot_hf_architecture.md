# Arquitectura Chatbot con Modelo Hugging Face

## ğŸ—ï¸ DiseÃ±o del Sistema

### Stack TecnolÃ³gico Propuesto

#### OpciÃ³n A: AplicaciÃ³n Web Moderna
```
Frontend:
â”œâ”€â”€ React 18 + TypeScript
â”œâ”€â”€ Tailwind CSS + Shadcn/ui
â”œâ”€â”€ Socket.io (tiempo real)
â””â”€â”€ Vite (build tool)

Backend:
â”œâ”€â”€ FastAPI + Python 3.11
â”œâ”€â”€ WebSockets para chat en tiempo real
â”œâ”€â”€ Transformers + PEFT
â”œâ”€â”€ SQLite/PostgreSQL
â””â”€â”€ Redis (cache opcional)

Deployment:
â”œâ”€â”€ Docker containers
â”œâ”€â”€ Nginx (reverse proxy)
â””â”€â”€ Cloud deployment ready
```

#### OpciÃ³n B: AplicaciÃ³n Desktop HÃ­brida
```
Frontend:
â”œâ”€â”€ Electron + React
â”œâ”€â”€ Tailwind CSS
â””â”€â”€ IPC communication

Backend:
â”œâ”€â”€ Python subprocess
â”œâ”€â”€ Transformers + PEFT
â”œâ”€â”€ Local SQLite
â””â”€â”€ File-based cache
```

## ğŸ¤– IntegraciÃ³n del Modelo

### Modelo: zementalist/llama-3-8B-chat-psychotherapist

**CaracterÃ­sticas:**
- **TamaÃ±o**: 4.65B parÃ¡metros
- **Tipo**: PEFT (Parameter Efficient Fine-Tuning)
- **EspecializaciÃ³n**: Salud mental y psicoterapia
- **Licencia**: MIT (uso libre)

### ImplementaciÃ³n TÃ©cnica

```python
# core/hf_model_manager.py
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel, PeftConfig
import torch

class HuggingFaceModelManager:
    def __init__(self, model_id="zementalist/llama-3-8B-chat-psychotherapist"):
        self.model_id = model_id
        self.model = None
        self.tokenizer = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
    
    def load_model(self):
        """Carga el modelo PEFT optimizado"""
        config = PeftConfig.from_pretrained(self.model_id)
        
        # Cargar modelo base
        base_model = AutoModelForCausalLM.from_pretrained(
            config.base_model_name_or_path,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
            device_map="auto" if self.device == "cuda" else None
        )
        
        # Aplicar adaptadores PEFT
        self.model = PeftModel.from_pretrained(base_model, self.model_id)
        
        # Cargar tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)
        
        return True
    
    def generate_response(self, user_message: str, conversation_history: list = None):
        """Genera respuesta usando el modelo"""
        messages = [
            {"role": "system", "content": "Eres un asistente especializado en salud mental. Proporciona apoyo empÃ¡tico y profesional."},
            {"role": "user", "content": user_message}
        ]
        
        # Agregar historial si existe
        if conversation_history:
            messages = conversation_history + messages[-1:]
        
        # Aplicar template de chat
        input_ids = self.tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_tensors="pt"
        ).to(self.device)
        
        # Generar respuesta
        with torch.no_grad():
            outputs = self.model.generate(
                input_ids,
                max_new_tokens=256,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # Decodificar respuesta
        response = outputs[0][input_ids.shape[-1]:]
        decoded_response = self.tokenizer.decode(response, skip_special_tokens=True)
        
        return decoded_response.strip()
```

## ğŸŒ Arquitectura Web (OpciÃ³n A)

### Estructura del Proyecto
```
chatbot_hf_web/
â”œâ”€â”€ frontend/                 # React app
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatInterface.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ MessageBubble.tsx
â”‚   â”‚   â”‚   â””â”€â”€ TypingIndicator.tsx
â”‚   â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”‚   â”œâ”€â”€ useWebSocket.ts
â”‚   â”‚   â”‚   â””â”€â”€ useChat.ts
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ api.ts
â”‚   â”‚   â””â”€â”€ App.tsx
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ vite.config.ts
â”œâ”€â”€ backend/                  # FastAPI app
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”‚   â”œâ”€â”€ hf_model_manager.py
â”‚   â”‚   â”‚   â”œâ”€â”€ conversation_manager.py
â”‚   â”‚   â”‚   â””â”€â”€ config.py
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”œâ”€â”€ chat.py
â”‚   â”‚   â”‚   â””â”€â”€ websocket.py
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â””â”€â”€ schemas.py
â”‚   â”‚   â””â”€â”€ main.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

### API Endpoints

```python
# backend/app/api/chat.py
from fastapi import APIRouter, WebSocket, WebSocketDisconnect
from app.core.hf_model_manager import HuggingFaceModelManager
from app.core.conversation_manager import ConversationManager

router = APIRouter()
model_manager = HuggingFaceModelManager()
conversation_manager = ConversationManager()

@router.websocket("/ws/chat/{session_id}")
async def websocket_chat(websocket: WebSocket, session_id: str):
    await websocket.accept()
    
    try:
        while True:
            # Recibir mensaje del usuario
            data = await websocket.receive_json()
            user_message = data.get("message", "")
            
            # Obtener historial de conversaciÃ³n
            history = conversation_manager.get_history(session_id)
            
            # Generar respuesta con el modelo
            response = model_manager.generate_response(user_message, history)
            
            # Guardar en historial
            conversation_manager.add_message(session_id, "user", user_message)
            conversation_manager.add_message(session_id, "assistant", response)
            
            # Enviar respuesta
            await websocket.send_json({
                "type": "response",
                "message": response,
                "timestamp": datetime.now().isoformat()
            })
            
    except WebSocketDisconnect:
        conversation_manager.close_session(session_id)
```

### Frontend React

```tsx
// frontend/src/components/ChatInterface.tsx
import React, { useState, useEffect } from 'react';
import { useWebSocket } from '../hooks/useWebSocket';

interface Message {
  id: string;
  type: 'user' | 'assistant';
  content: string;
  timestamp: string;
}

export const ChatInterface: React.FC = () => {
  const [messages, setMessages] = useState<Message[]>([]);
  const [inputValue, setInputValue] = useState('');
  const [isTyping, setIsTyping] = useState(false);
  
  const { sendMessage, lastMessage, connectionStatus } = useWebSocket();
  
  useEffect(() => {
    if (lastMessage) {
      const data = JSON.parse(lastMessage.data);
      if (data.type === 'response') {
        setMessages(prev => [...prev, {
          id: Date.now().toString(),
          type: 'assistant',
          content: data.message,
          timestamp: data.timestamp
        }]);
        setIsTyping(false);
      }
    }
  }, [lastMessage]);
  
  const handleSendMessage = () => {
    if (inputValue.trim()) {
      // Agregar mensaje del usuario
      const userMessage = {
        id: Date.now().toString(),
        type: 'user' as const,
        content: inputValue,
        timestamp: new Date().toISOString()
      };
      
      setMessages(prev => [...prev, userMessage]);
      setIsTyping(true);
      
      // Enviar al backend
      sendMessage({ message: inputValue });
      setInputValue('');
    }
  };
  
  return (
    <div className="flex flex-col h-screen bg-gray-50">
      {/* Header */}
      <div className="bg-blue-600 text-white p-4">
        <h1 className="text-xl font-semibold">Asistente de Salud Mental</h1>
        <p className="text-blue-100">
          Estado: {connectionStatus === 'Open' ? 'Conectado' : 'Desconectado'}
        </p>
      </div>
      
      {/* Messages */}
      <div className="flex-1 overflow-y-auto p-4 space-y-4">
        {messages.map((message) => (
          <MessageBubble key={message.id} message={message} />
        ))}
        {isTyping && <TypingIndicator />}
      </div>
      
      {/* Input */}
      <div className="border-t bg-white p-4">
        <div className="flex space-x-2">
          <input
            type="text"
            value={inputValue}
            onChange={(e) => setInputValue(e.target.value)}
            onKeyPress={(e) => e.key === 'Enter' && handleSendMessage()}
            placeholder="Escribe tu mensaje..."
            className="flex-1 border rounded-lg px-3 py-2 focus:outline-none focus:ring-2 focus:ring-blue-500"
          />
          <button
            onClick={handleSendMessage}
            disabled={!inputValue.trim() || connectionStatus !== 'Open'}
            className="bg-blue-600 text-white px-4 py-2 rounded-lg hover:bg-blue-700 disabled:opacity-50"
          >
            Enviar
          </button>
        </div>
      </div>
    </div>
  );
};
```

## ğŸ–¥ï¸ Arquitectura Desktop (OpciÃ³n B)

### Estructura del Proyecto
```
chatbot_hf_desktop/
â”œâ”€â”€ electron/                 # Electron main process
â”‚   â”œâ”€â”€ main.js
â”‚   â”œâ”€â”€ preload.js
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ frontend/                 # React renderer
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â””â”€â”€ App.tsx
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ backend/                  # Python backend
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ hf_model_manager.py
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ build/                    # Build scripts
â”‚   â”œâ”€â”€ build.js
â”‚   â””â”€â”€ package.py
â””â”€â”€ dist/                     # Distribution files
```

### ComunicaciÃ³n IPC

```javascript
// electron/main.js
const { app, BrowserWindow, ipcMain } = require('electron');
const { spawn } = require('child_process');

let pythonProcess;

function createWindow() {
  const mainWindow = new BrowserWindow({
    width: 1200,
    height: 800,
    webPreferences: {
      nodeIntegration: false,
      contextIsolation: true,
      preload: path.join(__dirname, 'preload.js')
    }
  });
  
  // Iniciar proceso Python
  pythonProcess = spawn('python', ['backend/main.py']);
  
  mainWindow.loadFile('frontend/dist/index.html');
}

ipcMain.handle('send-message', async (event, message) => {
  return new Promise((resolve) => {
    // ComunicaciÃ³n con Python backend
    pythonProcess.stdin.write(JSON.stringify({ message }) + '\n');
    
    pythonProcess.stdout.once('data', (data) => {
      const response = JSON.parse(data.toString());
      resolve(response);
    });
  });
});
```

## âš¡ Optimizaciones de Rendimiento

### 1. Carga Lazy del Modelo
```python
class LazyModelManager:
    def __init__(self):
        self._model = None
        self._tokenizer = None
    
    @property
    def model(self):
        if self._model is None:
            self._load_model()
        return self._model
    
    def _load_model(self):
        # Carga el modelo solo cuando se necesita
        pass
```

### 2. Cache de Respuestas
```python
import hashlib
import json
from functools import lru_cache

class ResponseCache:
    def __init__(self, max_size=1000):
        self.cache = {}
        self.max_size = max_size
    
    def get_cache_key(self, message: str, context: list = None):
        content = message + str(context or [])
        return hashlib.md5(content.encode()).hexdigest()
    
    def get(self, message: str, context: list = None):
        key = self.get_cache_key(message, context)
        return self.cache.get(key)
    
    def set(self, message: str, response: str, context: list = None):
        key = self.get_cache_key(message, context)
        if len(self.cache) >= self.max_size:
            # Eliminar entrada mÃ¡s antigua
            oldest_key = next(iter(self.cache))
            del self.cache[oldest_key]
        self.cache[key] = response
```

### 3. CuantizaciÃ³n del Modelo
```python
from transformers import BitsAndBytesConfig

# ConfiguraciÃ³n para 4-bit quantization
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map="auto"
)
```

## ğŸ“Š ComparaciÃ³n de Opciones

| CaracterÃ­stica | Web App | Desktop App |
|----------------|---------|-------------|
| **Accesibilidad** | âœ… Cualquier navegador | âŒ InstalaciÃ³n requerida |
| **Rendimiento** | âš ï¸ Depende de servidor | âœ… Local, mÃ¡s rÃ¡pido |
| **Escalabilidad** | âœ… MÃºltiples usuarios | âŒ Un usuario por instalaciÃ³n |
| **Mantenimiento** | âœ… Actualizaciones centralizadas | âŒ Actualizaciones manuales |
| **Recursos** | âœ… Servidor compartido | âŒ Recursos locales |
| **Privacidad** | âš ï¸ Datos en servidor | âœ… Todo local |
| **DistribuciÃ³n** | âœ… URL simple | âŒ Ejecutables por OS |

## ğŸš€ Plan de ImplementaciÃ³n

### Fase 1: PreparaciÃ³n (1-2 dÃ­as)
1. âœ… AnÃ¡lisis del modelo HF completado
2. ğŸ”„ CorrecciÃ³n del dataset JSON (en progreso)
3. â³ DecisiÃ³n de arquitectura (web vs desktop)

### Fase 2: Desarrollo Core (3-5 dÃ­as)
1. Implementar HuggingFaceModelManager
2. Crear sistema de conversaciones
3. Desarrollar API/IPC segÃºn arquitectura elegida

### Fase 3: Frontend (2-3 dÃ­as)
1. DiseÃ±ar interfaz moderna
2. Implementar chat en tiempo real
3. Agregar funcionalidades avanzadas

### Fase 4: IntegraciÃ³n y Testing (1-2 dÃ­as)
1. Pruebas de rendimiento
2. Optimizaciones
3. DocumentaciÃ³n

## ğŸ¯ PrÃ³ximos Pasos

1. **Decidir arquitectura**: Â¿Web o Desktop?
2. **Configurar entorno**: Instalar dependencias HF
3. **Implementar modelo manager**: CÃ³digo base para el modelo
4. **Crear interfaz bÃ¡sica**: Chat funcional mÃ­nimo

Â¿QuÃ© arquitectura prefieres para empezar?
